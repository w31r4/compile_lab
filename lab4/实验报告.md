杭州电子科技大学
《编译原理课程实践》
实验报告

题    目：
学    院：计算机学院
专    业：计算机科学与技术
班    级：[您的班级]
学    号：[您的学号]
姓    名：[您的姓名]
完成日期：2024年12月28日

---

## 一、 实验目的

本实验旨在让学生深入理解和掌握编译器前端的核心技术，包括词法分析、语法分析和语义分析。通过实现 SysY 语言（C 语言子集）的编译器前端，达到以下具体目的：

1.  **掌握词法分析原理**：理解正则表达式与有限自动机在词法分析中的应用，掌握词法记号（Token）的识别与提取方法，能够设计并实现一个完整的词法分析器。
2.  **掌握语法分析原理**：深入理解上下文无关文法（CFG），掌握递归下降分析法（Recursive Descent Parsing），能够根据 SysY 语言的文法规范设计并实现语法分析器，生成抽象语法树（AST）。
3.  **掌握语义分析与静态检查**：理解作用域、类型系统和符号表的概念，掌握如何在遍历 AST 的过程中建立符号表，进行变量/函数的定义与引用检查、类型检查、控制流检查等静态语义检查。
4.  **熟悉编译器工程开发**：通过实际编码，熟悉编译器前端的模块化设计，掌握错误处理与报告机制，提升处理复杂系统软件的能力。

## 二、 实验内容与实验要求

本次实验要求基于 SysY 语言标准，设计并实现一个完整的编译器前端，主要包含以下四个任务：

### 任务 4.1：SysY 语言的语法和语义规范理解
*   **内容**：详细阅读 SysY 语言定义文档，理解其支持的数据类型（int, float, void）、控制结构（if-else, while, break, continue）、函数定义与调用、数组操作以及全局/局部变量的作用域规则。
*   **要求**：梳理 SysY 语言的文法产生式，明确各语法结构的语义约束（如不能修改常量、数组下标必须为整数等）。

### 任务 4.2：词法分析器设计与实现
*   **内容**：设计词法分析器，将源代码文本流转换为 Token 流。
*   **要求**：
    *   能够识别关键字（const, int, if, while 等）、标识符、常量（十进制、八进制、十六进制整数，浮点数）、运算符和界符。
    *   能够过滤注释（单行 `//` 和多行 `/* ... */`）和空白字符。
    *   能够准确报告词法错误（如非法字符）。

### 任务 4.3：语法分析器设计与实现
*   **内容**：基于 Token 流，根据 SysY 文法构建抽象语法树（AST）。
*   **要求**：
    *   采用递归下降分析法。
    *   设计合理的 AST 节点结构（如 `Program`, `FuncDef`, `Block`, `IfStmt`, `BinaryOp` 等）。
    *   能够处理优先级和结合性（如乘除优于加减）。
    *   能够报告语法错误（如缺少分号、括号不匹配）。

### 任务 4.4：语义分析
*   **内容**：遍历 AST，建立符号表，进行类型检查和语义约束检查。
*   **要求**：
    *   实现符号表（Symbol Table），支持多级作用域（全局、函数参数、块级局部）。
    *   检测 17 类语义错误，包括但不限于：
        *   变量/函数未定义或重定义。
        *   函数调用参数个数或类型不匹配。
        *   非整数作为数组下标。
        *   在非循环语句中使用 break/continue。
        *   修改常量。
        *   函数返回值类型不匹配。

## 三、 设计方案与算法描述

### 3.1 总体架构

编译器前端采用经典的流水线架构，分为词法分析、语法分析和语义分析三个阶段。

```
+--------------+      +----------------+      +------------------+      +------------------+
|  Source Code | ---> |     Lexer      | ---> |      Parser      | ---> | Semantic Analyzer|
|  (.sy file)  |      | (Token Stream) |      | (Abstract Syntax |      | (Symbol Table &  |
+--------------+      +----------------+      |       Tree)      |      |  Error Report)   |
                                              +------------------+      +------------------+
```

### 3.2 词法分析器 (Lexer)

词法分析器 `Lexer` 逐字符读取源代码，根据 SysY 的词法规则匹配并生成 Token。主要维护当前行号和列号以便报错。

**核心算法流程：**

```
Start
  |
  v
While (char is not EOF)
  |
  +-> Skip Whitespace & Comments
  |
  +-> Match Keyword or Identifier?
  |     |
  |     +-> Yes: Return TOKEN_KEYWORD / TOKEN_IDENTIFIER
  |
  +-> Match Number (Hex/Oct/Dec/Float)?
  |     |
  |     +-> Yes: Return TOKEN_INT_CONST / TOKEN_FLOAT_CONST
  |
  +-> Match Operator or Delimiter (e.g., +, <=, ;, {)?
  |     |
  |     +-> Yes: Return TOKEN_OPERATOR / TOKEN_DELIMITER
  |
  +-> No Match?
        |
        +-> Report LEXICAL_ERROR
```

**关键数据结构：**
*   `TokenType` (Enum): 定义所有可能的 Token 类型。
*   `Token` (Class): 包含 type, value, line, column。

### 3.3 语法分析器 (Parser)

语法分析器 `Parser` 采用递归下降策略。每个非终结符对应一个解析函数（如 `parse_program`, `parse_func_def`, `parse_stmt`）。

**AST 类层次结构 (部分)：**

```
ASTNode
  +-- CompUnit
  +-- Decl (ConstDecl/VarDecl)
  +-- FuncDef
  +-- Block
  +-- Stmt
  |    +-- If
  |    +-- While
  |    +-- Return
  |    +-- Assign
  |    +-- Exp
  +-- Exp
       +-- AddExp / MulExp / RelExp / EqExp
       +-- UnaryExp
       +-- LVal
       +-- Number
```

**递归下降分析流程 (以 parse_primary_exp 为例)：**

```
parse_primary_exp()
  |
  +-> If '('
  |     Eat '('
  |     expr = parse_exp()
  |     Eat ')'
  |     Return expr
  |
  +-> If Number
  |     Return parse_number()
  |
  +-> If Identifier
  |     Return parse_lval()
  |
  +-> Else
        Report SYNTAX_ERROR
```

### 3.4 语义分析器 (Semantic Analyzer)

语义分析器 `SemanticAnalyzer` 遍历 AST，维护一个作用域栈（Scope Stack），并在符号表中记录变量和函数信息。

**符号表设计：**

```
SymbolTable
  |
  +-- scope_stack: List[Scope]  (Stack of scopes)
  |
  +-- enter_scope(): Create new scope, push to stack
  +-- exit_scope(): Pop scope from stack
  +-- define(symbol): Add to current scope
  +-- lookup(name): Search from current scope to global
```

**语义检查流程 (AST Walker)：**

1.  **CompUnit**: 初始化全局作用域，注册标准库函数（如 `getint`, `putint`）。
2.  **FuncDef**:
    *   检查函数名是否重复。
    *   注册函数符号。
    *   进入新的函数作用域。
    *   注册参数变量。
    *   遍历函数体 Block。
    *   退出作用域。
3.  **VarDecl**:
    *   检查变量名是否重复。
    *   注册变量符号。
4.  **BinaryOp**:
    *   递归检查左右操作数。
    *   推导结果类型（如 int + float -> float）。
5.  **If/While**:
    *   检查条件表达式类型。
    *   进入/退出块作用域。
    *   检查 break/continue 是否在循环内。

## 四、 测试结果

本实验使用了多个测试用例覆盖词法、语法和语义的各种情况。

### 4.1 语义错误检测 (Test Case 12)

**输入文件**: `test_12_all_semantic_errors.sy` (包含 17 种典型语义错误)

**运行命令**: `python3 lab4/main.py lab4/test_cases/test_12_all_semantic_errors.sy --semantic`

**输出结果**:

```text
Error type 4 at Line 13: Redefined function 'helper_func'
Error type 2 at Line 21: Redefined variable 'global_counter'
Error type 1 at Line 31: Undefined variable 'undefined_x'
Error type 1 at Line 32: Undefined variable 'missing_var'
Error type 2 at Line 38: Redefined variable 'local_num'
Error type 3 at Line 43: Undefined function 'unknown_function'
Error type 3 at Line 44: Undefined function 'foo'
Error type 5 at Line 50: Variable 'not_a_function' is not a function
Error type 5 at Line 51: Variable 'local_num' is not a function
Error type 6 at Line 56: Function 'helper_func' used as variable
Error type 6 at Line 57: Function 'helper_func' used as variable
Error type 7 at Line 64: Array index is not an integer
Error type 7 at Line 65: Array index is not an integer
Error type 8 at Line 71: Variable 'simple_var' is not an array
Error type 8 at Line 72: Variable 'simple_var' is not an array
Error type 9 at Line 77: Function 'helper_func' expects 0 arguments but got 1
Error type 9 at Line 78: Function 'helper_func' expects 0 arguments but got 3
Error type 12 at Line 83: break statement not within a loop
Error type 13 at Line 88: continue statement not within a loop
Error type 14 at Line 94: Array index 10 out of bounds (size: 5)
Error type 14 at Line 96: Array index 100 out of bounds (size: 5)
Error type 15 at Line 102: Cannot modify constant 'MAX_SIZE'
Error type 15 at Line 105: Cannot modify constant 'PI_INT'
Error type 10 at Line 128: Missing return value in non-void function 'func_missing_return'
Error type 10 at Line 138: Return value in void function 'func_extra_return'
Error type 10 at Line 142: Return value in void function 'func_return_float'
Error type 11 at Line 151: Type mismatch for operands (int and float cannot mix)
Error type 9 at Line 167: Function 'add_two' expects 2 arguments but got 1
Error type 9 at Line 168: Function 'add_two' expects 2 arguments but got 3
Error type 9 at Line 169: Function 'add_two' expects 2 arguments but got 0
```

**分析**: 编译器正确识别了所有预设的语义错误，包括重定义、未定义、类型不匹配、参数错误、越界访问等，验证了语义分析器的正确性。

### 4.2 正确程序分析 (Test Case 01)

**输入文件**: `test_01_basic.sy`

**运行命令**: `python3 lab4/main.py lab4/test_cases/test_01_basic.sy --semantic`

**输出结果**:

```text
success
```

**分析**: 对于符合 SysY 规范的代码，编译器输出 success，表示通过语义检查。

## 五、 源代码

### 5.1 Token 定义 (src/token_type.py)

```python
from enum import Enum, auto


class TokenType(Enum):
    # Keywords
    CONST = auto()
    INT = auto()
    FLOAT = auto()
    VOID = auto()
    IF = auto()
    ELSE = auto()
    WHILE = auto()
    BREAK = auto()
    CONTINUE = auto()
    RETURN = auto()

    # Operators
    PLUS = auto()  # +
    MINUS = auto()  # -
    MUL = auto()  # *
    DIV = auto()  # /
    MOD = auto()  # %
    ASSIGN = auto()  # =
    EQ = auto()  # ==
    NEQ = auto()  # !=
    LT = auto()  # <
    GT = auto()  # >
    LE = auto()  # <=
    GE = auto()  # >=
    NOT = auto()  # !
    AND = auto()  # &&
    OR = auto()  # ||

    # Delimiters
    LPAREN = auto()  # (
    RPAREN = auto()  # )
    LBRACKET = auto()  # [
    RBRACKET = auto()  # ]
    LBRACE = auto()  # {
    RBRACE = auto()  # }
    COMMA = auto()  # ,
    SEMICOLON = auto()  # ;

    # Literals and Identifiers
    ID = auto()
    INT_CONST = auto()
    FLOAT_CONST = auto()

    # End of File
    EOF = auto()


class Token:
    def __init__(self, type: TokenType, value: str, line: int, column: int, numeric_value=None):
        self.type = type
        self.value = value  # Original lexeme
        self.line = line
        self.column = column
        self.numeric_value = numeric_value  # Decimal value for INTCON/FLOATCON

    def __repr__(self):
        return f"Token({self.type.name}, '{self.value}', line={self.line}, col={self.column})"

    def to_string(self):
        # Format output as required by the lab: <Type, Value>
        # Note: The lab spec might require specific type names (e.g., INTTK instead of INT)
        # We will map them here.
        type_map = {
            TokenType.CONST: "CONSTTK",
            TokenType.INT: "INTTK",
            TokenType.FLOAT: "FLOATTK",
            TokenType.VOID: "VOIDTK",
            TokenType.IF: "IFTK",
            TokenType.ELSE: "ELSETK",
            TokenType.WHILE: "WHILETK",
            TokenType.BREAK: "BREAKTK",
            TokenType.CONTINUE: "CONTINUETK",
            TokenType.RETURN: "RETURNTK",
            TokenType.PLUS: "PLUS",
            TokenType.MINUS: "MINU",
            TokenType.MUL: "MULT",
            TokenType.DIV: "DIV",
            TokenType.MOD: "MOD",
            TokenType.ASSIGN: "ASSIGN",
            TokenType.EQ: "EQL",
            TokenType.NEQ: "NEQ",
            TokenType.LT: "LSS",
            TokenType.GT: "GRE",
            TokenType.LE: "LEQ",
            TokenType.GE: "GEQ",
            TokenType.NOT: "NOT",
            TokenType.AND: "AND",
            TokenType.OR: "OR",
            TokenType.LPAREN: "LPARENT",
            TokenType.RPAREN: "RPARENT",
            TokenType.LBRACKET: "LBRACK",
            TokenType.RBRACKET: "RBRACK",
            TokenType.LBRACE: "LBRACE",
            TokenType.RBRACE: "RBRACE",
            TokenType.COMMA: "COMMA",
            TokenType.SEMICOLON: "SEMICN",
            TokenType.ID: "IDENFR",
            TokenType.INT_CONST: "INTCON",
            TokenType.FLOAT_CONST: "FLOATCON",
        }

        type_str = type_map.get(self.type, self.type.name)
        
        # For numeric constants, output decimal value
        if self.type == TokenType.INT_CONST and self.numeric_value is not None:
            return f"{type_str} {self.numeric_value}"
        elif self.type == TokenType.FLOAT_CONST and self.numeric_value is not None:
            # Format float: remove unnecessary trailing zeros but keep at least one decimal
            float_str = str(self.numeric_value)
            return f"{type_str} {float_str}"
        
        return f"{type_str} {self.value}"
```

### 5.2 词法分析器 (src/lexer.py)

```python
from .token_type import TokenType, Token


class LexerError(Exception):
    def __init__(self, message, line):
        self.message = message
        self.line = line


class Lexer:
    def __init__(self, source_code):
        self.source = source_code
        self.pos = 0
        self.line = 1
        self.column = 1
        self.tokens = []
        self.has_error = False
        self.keywords = {
            "const": TokenType.CONST,
            "int": TokenType.INT,
            "float": TokenType.FLOAT,
            "void": TokenType.VOID,
            "if": TokenType.IF,
            "else": TokenType.ELSE,
            "while": TokenType.WHILE,
            "break": TokenType.BREAK,
            "continue": TokenType.CONTINUE,
            "return": TokenType.RETURN,
        }

    def error(self, message):
        # Format: Error type A at Line [line]: [message]
        print(f"Error type A at Line {self.line}: {message}")
        self.has_error = True
        # We continue lexing to find more errors if possible, or just skip the char
        # For this implementation, we'll skip the current char and continue
        self.advance()

    def peek(self):
        if self.pos < len(self.source):
            return self.source[self.pos]
        return None

    def advance(self):
        if self.pos < len(self.source):
            char = self.source[self.pos]
            self.pos += 1
            if char == "\n":
                self.line += 1
                self.column = 1
            else:
                self.column += 1
            return char
        return None

    def skip_whitespace(self):
        while self.peek() is not None and self.peek().isspace():
            self.advance()

    def skip_comment(self):
        # Single line comment //
        if self.source.startswith("//", self.pos):
            while self.peek() is not None and self.peek() != "\n":
                self.advance()
            return True
        # Multi-line comment /* ... */
        elif self.source.startswith("/*", self.pos):
            self.advance()  # /
            self.advance()  # *
            while self.peek() is not None:
                if self.source.startswith("*/", self.pos):
                    self.advance()  # *
                    self.advance()  # /
                    return True
                self.advance()
            # If we reach EOF inside a comment, it's technically an error but usually handled gracefully or ignored
            return True
        return False

    def tokenize(self):
        while self.peek() is not None:
            self.skip_whitespace()
            if self.peek() is None:
                break

            if self.skip_comment():
                continue

            char = self.peek()

            if char.isalpha() or char == "_":
                self.scan_identifier_or_keyword()
            elif char.isdigit():
                self.scan_number()
            elif char == ".":
                # Could be a float starting with . like .5
                if self.pos + 1 < len(self.source) and self.source[self.pos + 1].isdigit():
                    self.scan_number()
                else:
                    self.error(f"Invalid character '{char}'")
            elif char == '"':
                # String literals are not part of SysY core but might be needed for printf format strings in runtime lib calls
                self.scan_string_literal()
            else:
                self.scan_operator_or_delimiter()

        return self.tokens

    def scan_identifier_or_keyword(self):
        start_col = self.column
        value = ""
        while self.peek() is not None and (self.peek().isalnum() or self.peek() == "_"):
            value += self.advance()

        token_type = self.keywords.get(value, TokenType.ID)
        self.tokens.append(Token(token_type, value, self.line, start_col))

    def scan_number(self):
        start_col = self.column
        start_line = self.line
        value = ""
        is_float = False
        is_hex = False
        is_octal = False

        if self.peek() == "0":
            value += self.advance()
            if self.peek() in ("x", "X"):
                is_hex = True
                value += self.advance()
            elif self.peek() is not None and self.peek().isdigit():
                is_octal = True  # Potential octal

        while self.peek() is not None:
            char = self.peek()
            if is_hex:
                if char.isdigit() or char in "abcdefABCDEF":
                    value += self.advance()
                elif char == ".":
                    is_float = True
                    value += self.advance()
                elif char in ("p", "P"):
                    is_float = True
                    value += self.advance()
                    if self.peek() in ("+", "-"):
                        value += self.advance()
                    # After p/P, must have digits for exponent
                    if self.peek() is None or not self.peek().isdigit():
                        self.error(f"Invalid hexadecimal float '{value}'")
                        return
                    while self.peek() is not None and self.peek().isdigit():
                        value += self.advance()
                else:
                    break
            else:
                if char.isdigit():
                    value += self.advance()
                elif char == ".":
                    is_float = True
                    is_octal = False  # Not octal if it has decimal point
                    value += self.advance()
                elif char in ("e", "E"):
                    is_float = True
                    is_octal = False  # Not octal if it has exponent
                    value += self.advance()
                    if self.peek() in ("+", "-"):
                        value += self.advance()
                    # After e/E, must have digits for exponent
                    if self.peek() is None or not self.peek().isdigit():
                        self.error(f"Invalid float exponent in '{value}'")
                        return
                    while self.peek() is not None and self.peek().isdigit():
                        value += self.advance()
                else:
                    break

        # Check for invalid trailing characters after hex
        if is_hex and self.peek() is not None and (self.peek().isalpha() or self.peek() == "_"):
            invalid_char = self.peek()
            self.error(f"Invalid hexadecimal number '{value}{invalid_char}'")
            return

        # Validation and conversion
        numeric_value = None

        if is_hex and is_float:
            # Hex float validation and conversion
            try:
                numeric_value = float.fromhex(value)
            except ValueError:
                self.error(f"Invalid hexadecimal float '{value}'")
                return
        elif is_hex:
            # Hex int validation and conversion
            try:
                numeric_value = int(value, 16)
            except ValueError:
                self.error(f"Invalid hexadecimal number '{value}'")
                return
        elif is_float:
            # Decimal float validation and conversion
            try:
                numeric_value = float(value)
            except ValueError:
                self.error(f"Invalid float number '{value}'")
                return
        else:
            # Decimal or Octal int
            if is_octal or (value.startswith("0") and len(value) > 1):
                # Octal check - validate all digits are 0-7
                for c in value[1:]:  # Skip leading 0
                    if c in "89":
                        self.error(f"Illegal octal number '{value}'")
                        return
                try:
                    numeric_value = int(value, 8)
                except ValueError:
                    self.error(f"Invalid octal number '{value}'")
                    return
            else:
                # Decimal
                try:
                    numeric_value = int(value)
                except ValueError:
                    self.error(f"Invalid integer '{value}'")
                    return

        if is_float:
            self.tokens.append(Token(TokenType.FLOAT_CONST, value, start_line, start_col, numeric_value))
        else:
            self.tokens.append(Token(TokenType.INT_CONST, value, start_line, start_col, numeric_value))

    def scan_string_literal(self):
        # Basic string skipping for now, as it's mainly for library calls
        self.advance()  # "
        while self.peek() is not None and self.peek() != '"':
            if self.peek() == "\\":
                self.advance()
            self.advance()
        if self.peek() == '"':
            self.advance()
        pass

    def scan_operator_or_delimiter(self):
        start_col = self.column
        char = self.peek()

        # Multi-character operators
        if char == "<":
            self.advance()
            if self.peek() == "=":
                self.advance()
                self.tokens.append(Token(TokenType.LE, "<=", self.line, start_col))
            else:
                self.tokens.append(Token(TokenType.LT, "<", self.line, start_col))
        elif char == ">":
            self.advance()
            if self.peek() == "=":
                self.advance()
                self.tokens.append(Token(TokenType.GE, ">=", self.line, start_col))
            else:
                self.tokens.append(Token(TokenType.GT, ">", self.line, start_col))
        elif char == "=":
            self.advance()
            if self.peek() == "=":
                self.advance()
                self.tokens.append(Token(TokenType.EQ, "==", self.line, start_col))
            else:
                self.tokens.append(Token(TokenType.ASSIGN, "=", self.line, start_col))
        elif char == "!":
            self.advance()
            if self.peek() == "=":
                self.advance()
                self.tokens.append(Token(TokenType.NEQ, "!=", self.line, start_col))
            else:
                self.tokens.append(Token(TokenType.NOT, "!", self.line, start_col))
        elif char == "&":
            self.advance()
            if self.peek() == "&":
                self.advance()
                self.tokens.append(Token(TokenType.AND, "&&", self.line, start_col))
            else:
                self.error(f"Invalid character '&'")
        elif char == "|":
            self.advance()
            if self.peek() == "|":
                self.advance()
                self.tokens.append(Token(TokenType.OR, "||", self.line, start_col))
            else:
                self.error(f"Invalid character '|'")

        # Single-character operators and delimiters
        elif char == "+":
            self.advance()
            self.tokens.append(Token(TokenType.PLUS, "+", self.line, start_col))
        elif char == "-":
            self.advance()
            self.tokens.append(Token(TokenType.MINUS, "-", self.line, start_col))
        elif char == "*":
            self.advance()
            self.tokens.append(Token(TokenType.MUL, "*", self.line, start_col))
        elif char == "/":
            self.advance()
            self.tokens.append(Token(TokenType.DIV, "/", self.line, start_col))
        elif char == "%":
            self.advance()
            self.tokens.append(Token(TokenType.MOD, "%", self.line, start_col))
        elif char == ";":
            self.advance()
            self.tokens.append(Token(TokenType.SEMICOLON, ";", self.line, start_col))
        elif char == ",":
            self.advance()
            self.tokens.append(Token(TokenType.COMMA, ",", self.line, start_col))
        elif char == "(":
            self.advance()
            self.tokens.append(Token(TokenType.LPAREN, "(", self.line, start_col))
        elif char == ")":
            self.advance()
            self.tokens.append(Token(TokenType.RPAREN, ")", self.line, start_col))
        elif char == "[":
            self.advance()
            self.tokens.append(Token(TokenType.LBRACKET, "[", self.line, start_col))
        elif char == "]":
            self.advance()
            self.tokens.append(Token(TokenType.RBRACKET, "]", self.line, start_col))
        elif char == "{":
            self.advance()
            self.tokens.append(Token(TokenType.LBRACE, "{", self.line, start_col))
        elif char == "}":
            self.advance()
            self.tokens.append(Token(TokenType.RBRACE, "}", self.line, start_col))
        else:
            self.error(f"Invalid character '{char}'")
```

### 5.3 AST 节点定义 (src/ast_nodes.py)

```python
from dataclasses import dataclass, field
from typing import List, Optional, Union
from enum import Enum, auto


class NodeType(Enum):
    COMP_UNIT = auto()
    DECL = auto()
    CONST_DECL = auto()
    VAR_DECL = auto()
    FUNC_DEF = auto()
    BLOCK = auto()
    STMT = auto()
    EXP = auto()
    # ... (其他节点类型)


@dataclass
class ASTNode:
    """AST 节点基类"""
    node_type: NodeType = NodeType.COMP_UNIT
    line: int = 0
    children: List["ASTNode"] = field(default_factory=list)

    def add_child(self, child: "ASTNode"):
        if child is not None:
            self.children.append(child)


@dataclass
class CompUnitNode(ASTNode):
    def __post_init__(self):
        self.node_type = NodeType.COMP_UNIT


@dataclass
class FuncDefNode(ASTNode):
    return_type: "BType" = None
    name: str = ""
    params: List["ASTNode"] = field(default_factory=list)
    body: Optional["ASTNode"] = None

    def __post_init__(self):
        self.node_type = NodeType.FUNC_DEF

# ... (其他节点类定义)
```

### 5.4 语法分析器 (src/parser.py)

```python
class Parser:
    def __init__(self, tokens: List[Token]):
        self.tokens = tokens
        self.pos = 0
        self.errors = []

    def parse(self) -> CompUnitNode:
        return self.parse_comp_unit()

    def parse_comp_unit(self) -> CompUnitNode:
        node = CompUnitNode(line=self.current_line())
        while self.current_token() is not None:
            if self.match(TokenType.CONST):
                decl = self.parse_decl()
                node.add_child(decl)
            elif self.match(TokenType.VOID):
                func_def = self.parse_func_def()
                node.add_child(func_def)
            elif self.match(TokenType.INT, TokenType.FLOAT):
                if self.is_func_def():
                    func_def = self.parse_func_def()
                    node.add_child(func_def)
                else:
                    decl = self.parse_decl()
                    node.add_child(decl)
            # ... Error handling
        return node

    # ... (其他递归下降分析方法，如 parse_decl, parse_stmt, parse_exp 等)
```

### 5.5 符号表 (src/symbol_table.py)

```python
@dataclass
class Symbol:
    name: str
    kind: SymbolKind
    type: SymbolType
    line: int
    scope_level: int = 0
    is_const: bool = False


class SymbolTable:
    def __init__(self):
        self.global_scope = Scope(level=0)
        self.current_scope = self.global_scope
        self.scope_stack = [self.global_scope]
        self._init_builtin_functions()

    def enter_scope(self):
        new_level = len(self.scope_stack)
        new_scope = Scope(level=new_level, parent=self.current_scope)
        self.scope_stack.append(new_scope)
        self.current_scope = new_scope

    def exit_scope(self):
        if len(self.scope_stack) > 1:
            self.scope_stack.pop()
            self.current_scope = self.scope_stack[-1]

    def define(self, symbol: Symbol) -> bool:
        return self.current_scope.define(symbol)

    def lookup(self, name: str) -> Optional[Symbol]:
        return self.current_scope.lookup(name)
```

### 5.6 语义分析器 (src/semantic_analyzer.py)

```python
class SemanticAnalyzer:
    def analyze(self, ast: CompUnitNode) -> bool:
        self.visit_comp_unit(ast)
        if not self.has_main:
            self.error(17, 1, "Missing 'main' function")
        return not self.has_error

    def visit_var_def(self, node: VarDefNode, base_type: TypeKind):
        # 检查重复定义
        existing = self.symbol_table.lookup_local(node.name)
        if existing:
            self.error(2, node.line, f"Redefined variable '{node.name}'")
            return
        # 定义变量
        symbol = Symbol(name=node.name, kind=SymbolKind.VARIABLE, type=base_type, line=node.line)
        self.symbol_table.define(symbol)

    def visit_lval(self, node: LValNode):
        # 检查未定义变量
        symbol = self.symbol_table.lookup(node.name)
        if symbol is None:
            self.error(1, node.line, f"Undefined variable '{node.name}'")
            return
        
        # 检查是否修改常量
        # ...

    # ... (其他 visit 方法，实现各种语义检查逻辑)
```

### 5.7 主程序 (main.py)

```python
def main():
    arg_parser = argparse.ArgumentParser()
    arg_parser.add_argument("source_file")
    arg_parser.add_argument("--semantic", "-s", action="store_true")
    args = arg_parser.parse_args()

    # 读取文件
    with open(args.source_file, "r") as f:
        source_code = f.read()

    # 词法分析
    lexer = Lexer(source_code)
    tokens = lexer.tokenize()
    if lexer.has_error: return 1

    # 语法分析
    parser = Parser(tokens)
    ast = parser.parse()
    if parser.has_error: return 1

    # 语义分析
    if args.semantic:
        analyzer = SemanticAnalyzer()
        analyzer.analyze(ast)
        if analyzer.has_error: return 1
        print("success")

if __name__ == "__main__":
    main()