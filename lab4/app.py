"""
SysY ç¼–è¯‘å™¨å‰ç«¯å¯è§†åŒ–æ¼”ç¤º

ä½¿ç”¨ Streamlit å±•ç¤ºè¯æ³•åˆ†æã€è¯­æ³•åˆ†æã€è¯­ä¹‰åˆ†æçš„å®Œæ•´æµç¨‹ã€‚
"""

import streamlit as st
import os
from io import StringIO
import sys

from src.lexer import Lexer, LexerError
from src.parser import Parser
from src.ast_nodes import ASTPrinter
from src.semantic_analyzer import SemanticAnalyzer


# é»˜è®¤ç¤ºä¾‹ä»£ç 
DEFAULT_CODE = """// SysY ç¤ºä¾‹ç¨‹åº
int main() {
    int student_id = 20220001;
    int a = 10;
    int b = 20;
    int sum = a + b;
    
    if (sum > 25) {
        return 1;
    } else {
        return 0;
    }
}
"""

# é¢„è®¾æµ‹è¯•ç”¨ä¾‹
TEST_CASES = {
    "åŸºç¡€æµ‹è¯• (test_01)": "test_cases/test_01_basic.sy",
    "ç®—æœ¯è¿ç®— (test_02)": "test_cases/test_02_arithmetic.sy",
    "æ§åˆ¶æµ (test_03)": "test_cases/test_03_control.sy",
    "å‡½æ•°å®šä¹‰ (test_04)": "test_cases/test_04_func.sy",
    "æ•°ç»„æ“ä½œ (test_05)": "test_cases/test_05_array.sy",
    "å¸¸é‡ä¸å…¨å±€ (test_06)": "test_cases/test_06_const_global.sy",
    "æµ®ç‚¹æ•° (test_07)": "test_cases/test_07_float.sy",
    "å¤æ‚ç¨‹åº (test_08)": "test_cases/test_08_complex.sy",
    "è¯æ³•é”™è¯¯ (test_09)": "test_cases/test_09_lex_error.sy",
    "è¯­æ³•é”™è¯¯ (test_10)": "test_cases/test_10_syntax_error.sy",
    "è¯­ä¹‰é”™è¯¯ (test_11)": "test_cases/test_11_semantic_error.sy",
    "â­ å…¨éƒ¨13ç§è¯­ä¹‰é”™è¯¯ (test_12)": "test_cases/test_12_all_semantic_errors.sy",
    "å…«è¿›åˆ¶æµ‹è¯•": "test_cases/test_octal.sy",
}


def load_test_file(filepath: str) -> str:
    """åŠ è½½æµ‹è¯•æ–‡ä»¶å†…å®¹"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        return f"// æ— æ³•åŠ è½½æ–‡ä»¶: {e}"


def run_lexer(source_code: str) -> tuple:
    """è¿è¡Œè¯æ³•åˆ†æå™¨ï¼Œè¿”å› (tokens, has_error, error_output)"""
    # æ•è·é”™è¯¯è¾“å‡º
    old_stdout = sys.stdout
    sys.stdout = StringIO()

    lexer = Lexer(source_code)
    tokens = []
    has_error = False

    try:
        tokens = lexer.tokenize()
        has_error = lexer.has_error
    except LexerError as e:
        has_error = True

    error_output = sys.stdout.getvalue()
    sys.stdout = old_stdout

    return tokens, has_error, error_output


def run_parser(tokens: list) -> tuple:
    """è¿è¡Œè¯­æ³•åˆ†æå™¨ï¼Œè¿”å› (ast, errors, ast_output, error_output)"""
    old_stdout = sys.stdout
    sys.stdout = StringIO()

    parser = Parser(tokens)
    ast = None
    ast_output = ""

    try:
        ast = parser.parse()
        if not parser.has_error:
            printer = ASTPrinter()
            ast_output = printer.print_ast(ast)
    except Exception as e:
        pass

    error_output = sys.stdout.getvalue()
    sys.stdout = old_stdout

    return ast, parser.errors, ast_output, error_output


def run_semantic(ast) -> tuple:
    """è¿è¡Œè¯­ä¹‰åˆ†æå™¨ï¼Œè¿”å› (success, errors, error_output)"""
    old_stdout = sys.stdout
    sys.stdout = StringIO()

    analyzer = SemanticAnalyzer()
    success = False

    try:
        success = analyzer.analyze(ast)
    except Exception as e:
        pass

    error_output = sys.stdout.getvalue()
    sys.stdout = old_stdout

    return success, analyzer.errors, error_output


def main():
    st.set_page_config(page_title="SysY ç¼–è¯‘å™¨å‰ç«¯æ¼”ç¤º", page_icon="ğŸ”§", layout="wide")

    st.title("ğŸ”§ SysY ç¼–è¯‘å™¨å‰ç«¯æ¼”ç¤º")
    st.caption("å®æ—¶å±•ç¤ºè¯æ³•åˆ†æã€è¯­æ³•åˆ†æã€è¯­ä¹‰åˆ†æçš„å®Œæ•´ç¼–è¯‘æµç¨‹")

    # é¡¶éƒ¨ï¼šæ”¯æŒçš„é”™è¯¯ç±»å‹è¯´æ˜
    with st.expander("ğŸ“‹ æ”¯æŒè¯†åˆ«çš„é”™è¯¯ç±»å‹ (å…±17ç§è¯­ä¹‰é”™è¯¯)", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            st.markdown(
                """
            **è¯æ³•é”™è¯¯ (Error type A)**
            - éæ³•å­—ç¬¦
            - æœªé—­åˆçš„æ³¨é‡Š
            - éæ³•çš„åå…­è¿›åˆ¶æµ®ç‚¹æ•° (å¦‚ 0x1.5p10)
            
            **è¯­æ³•é”™è¯¯ (Error type B)**
            - ç¼ºå°‘åˆ†å·ã€æ‹¬å·ç­‰
            - è¯­æ³•ç»“æ„ä¸å®Œæ•´
            - è¡¨è¾¾å¼è¯­æ³•é”™è¯¯
            
            **è¯­ä¹‰é”™è¯¯ (17ç§)**
            | ç±»å‹ | æè¿° |
            |------|------|
            | Error 1 | å˜é‡æœªå£°æ˜ |
            | Error 2 | å˜é‡é‡å¤å£°æ˜ |
            | Error 3 | è°ƒç”¨æœªå®šä¹‰çš„å‡½æ•° |
            | Error 4 | å‡½æ•°é‡å¤å®šä¹‰ |
            | Error 5 | æŠŠå˜é‡å½“åšå‡½æ•°è°ƒç”¨ |
            | Error 6 | å‡½æ•°åå½“æ™®é€šå˜é‡å¼•ç”¨ |
            | Error 7 | æ•°ç»„ä¸‹æ ‡ä¸æ˜¯æ•´å‹ |
            | Error 8 | éæ•°ç»„å˜é‡ä½¿ç”¨æ•°ç»„è®¿é—® |
            | Error 9 | å‡½æ•°å‚æ•°æ•°é‡æˆ–ç±»å‹ä¸åŒ¹é… |
            """
            )
        with col2:
            st.markdown(
                """
            | ç±»å‹ | æè¿° |
            |------|------|
            | Error 10 | returnç±»å‹ä¸å‡½æ•°è¿”å›ç±»å‹ä¸åŒ¹é… |
            | Error 11 | æ“ä½œæ•°ç±»å‹ä¸åŒ¹é… |
            | Error 12 | breakè¯­å¥ä¸åœ¨å¾ªç¯ä½“å†… |
            | Error 13 | continueè¯­å¥ä¸åœ¨å¾ªç¯ä½“å†… |
            | **Error 14** | **æ•°ç»„è¶Šç•Œè®¿é—®** |
            | **Error 15** | **ä¿®æ”¹å¸¸é‡** |
            | **Error 16** | **voidå‡½æ•°è¿”å›å€¼è¢«ä½¿ç”¨** |
            | **Error 17** | **ç¼ºå°‘mainå‡½æ•°** |
            """
            )

    # ä¾§è¾¹æ  - æµ‹è¯•ç”¨ä¾‹é€‰æ‹©
    st.sidebar.header("ğŸ“ å¯¼å…¥æµ‹è¯•ç”¨ä¾‹")
    selected_test = st.sidebar.selectbox(
        "é€‰æ‹©é¢„è®¾æµ‹è¯•ç”¨ä¾‹", ["è‡ªå®šä¹‰è¾“å…¥"] + list(TEST_CASES.keys()), help="é€‰æ‹©åä»£ç å°†å¯¼å…¥ç¼–è¾‘å™¨ï¼Œå¯è‡ªç”±ä¿®æ”¹"
    )

    # å¯¼å…¥æŒ‰é’®
    if st.sidebar.button("ğŸ“¥ å¯¼å…¥åˆ°ç¼–è¾‘å™¨", use_container_width=True):
        if selected_test != "è‡ªå®šä¹‰è¾“å…¥":
            filepath = TEST_CASES[selected_test]
            st.session_state.source_code = load_test_file(filepath)
        else:
            st.session_state.source_code = DEFAULT_CODE

    # åˆå§‹åŒ– session state
    if "source_code" not in st.session_state:
        if selected_test == "è‡ªå®šä¹‰è¾“å…¥":
            st.session_state.source_code = DEFAULT_CODE
        else:
            filepath = TEST_CASES[selected_test]
            st.session_state.source_code = load_test_file(filepath)

    # ä¾§è¾¹æ  - åˆ†æé€‰é¡¹
    st.sidebar.header("âš™ï¸ åˆ†æé€‰é¡¹")
    show_lexer = st.sidebar.checkbox("æ˜¾ç¤ºè¯æ³•åˆ†æè¯¦æƒ…", value=False)
    show_parser = st.sidebar.checkbox("æ˜¾ç¤ºè¯­æ³•åˆ†æè¯¦æƒ…", value=False)
    show_semantic = st.sidebar.checkbox("æ˜¾ç¤ºè¯­ä¹‰åˆ†æè¯¦æƒ…", value=False)

    # ä¸»åŒºåŸŸ - ä»£ç ç¼–è¾‘å™¨ï¼ˆæ›´å¤§çš„è¾“å…¥æ¡†ï¼‰
    st.subheader("ğŸ“ æºä»£ç ç¼–è¾‘å™¨")
    source_code = st.text_area(
        "SysY æºä»£ç ",
        value=st.session_state.source_code,
        height=400,  # æ›´å¤§çš„é«˜åº¦
        help="åœ¨æ­¤è¾“å…¥æˆ–ç¼–è¾‘ SysY ä»£ç ï¼Œä¿®æ”¹åè‡ªåŠ¨é‡æ–°åˆ†æ",
    )

    # åŒæ­¥åˆ° session state
    st.session_state.source_code = source_code

    if not source_code.strip():
        st.info("è¯·åœ¨ä¸Šæ–¹è¾“å…¥ SysY ä»£ç ä»¥å¼€å§‹åˆ†æ")
        return

    # ========== å®æ—¶é”™è¯¯æ˜¾ç¤ºï¼ˆç´§è´´ä»£ç ç¼–è¾‘å™¨ä¸‹æ–¹ï¼‰==========
    all_errors = []

    # è¿è¡Œè¯æ³•åˆ†æ
    tokens, lex_has_error, lex_error_output = run_lexer(source_code)
    if lex_error_output:
        for line in lex_error_output.strip().split("\n"):
            if line.strip():
                all_errors.append(("A", line.strip()))

    # è¿è¡Œè¯­æ³•åˆ†æ
    ast = None
    parse_errors = []
    ast_output = ""
    parse_error_output = ""
    if tokens:
        ast, parse_errors, ast_output, parse_error_output = run_parser(tokens)
        if parse_error_output:
            for line in parse_error_output.strip().split("\n"):
                if line.strip():
                    all_errors.append(("B", line.strip()))

    # è¿è¡Œè¯­ä¹‰åˆ†æ
    semantic_errors = []
    semantic_error_output = ""
    semantic_success = True
    if ast and not parse_errors:
        semantic_success, semantic_errors, semantic_error_output = run_semantic(ast)
        if semantic_error_output:
            for line in semantic_error_output.strip().split("\n"):
                if line.strip():
                    all_errors.append(("è¯­ä¹‰", line.strip()))

    # æ˜¾ç¤ºé”™è¯¯é¢æ¿ï¼ˆåƒIDEä¸€æ ·ç´§è´´ç¼–è¾‘å™¨ä¸‹æ–¹ï¼‰
    if all_errors:
        st.markdown("---")
        st.markdown("### âŒ é—®é¢˜é¢æ¿")
        for err_type, err_msg in all_errors:
            if err_type == "A":
                st.error(f"ğŸ”¤ è¯æ³•é”™è¯¯: {err_msg}")
            elif err_type == "B":
                st.error(f"ğŸŒ³ è¯­æ³•é”™è¯¯: {err_msg}")
            else:
                st.warning(f"ğŸ” è¯­ä¹‰é”™è¯¯: {err_msg}")
    else:
        st.success("âœ… æ— é”™è¯¯ - ç¨‹åºåˆ†æé€šè¿‡!")

    # è¿è¡Œåˆ†æè¯¦æƒ…
    st.divider()

    # ========== è¯æ³•åˆ†æè¯¦æƒ… ==========
    if show_lexer:
        st.subheader("ğŸ”¤ ä»»åŠ¡ 4.2: è¯æ³•åˆ†æè¯¦æƒ…")

        if tokens:
            col1, col2 = st.columns([2, 1])

            with col1:
                # Token åˆ—è¡¨
                token_lines = []
                for token in tokens:
                    token_lines.append(token.to_string())

                with st.expander(f"Token åˆ—è¡¨ ({len(tokens)} ä¸ª)", expanded=False):
                    st.code("\n".join(token_lines), language="text")

            with col2:
                # ç»Ÿè®¡ä¿¡æ¯
                st.metric("Token æ•°é‡", len(tokens))

                # Token ç±»å‹åˆ†å¸ƒ
                type_count = {}
                for token in tokens:
                    t = token.type.name
                    type_count[t] = type_count.get(t, 0) + 1

                with st.expander("Token ç±»å‹åˆ†å¸ƒ"):
                    for t, count in sorted(type_count.items(), key=lambda x: -x[1])[:10]:
                        st.text(f"{t}: {count}")

        st.divider()

    # ========== è¯­æ³•åˆ†æè¯¦æƒ… ==========
    if show_parser:
        st.subheader("ğŸŒ³ ä»»åŠ¡ 4.3: è¯­æ³•åˆ†æè¯¦æƒ…")

        if ast_output:
            with st.expander("æŠ½è±¡è¯­æ³•æ ‘ (AST)", expanded=False):
                st.code(ast_output, language="text")

        st.divider()

    # ========== è¯­ä¹‰åˆ†æè¯¦æƒ… ==========
    if show_semantic:
        st.subheader("ğŸ” ä»»åŠ¡ 4.4: è¯­ä¹‰åˆ†æè¯¦æƒ…")

        if ast and not parse_errors:
            if semantic_success:
                st.info("ç¬¦å·è¡¨æ„å»ºæˆåŠŸï¼Œæ— è¯­ä¹‰é”™è¯¯")
            else:
                st.info(f"å‘ç° {len(semantic_errors)} ä¸ªè¯­ä¹‰é”™è¯¯ï¼ˆè¯¦è§ä¸Šæ–¹é—®é¢˜é¢æ¿ï¼‰")
        else:
            st.info("è¯­æ³•åˆ†ææœªå®Œæˆï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰åˆ†æ")

    # åº•éƒ¨è¯´æ˜
    st.divider()
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
        st.markdown(
            """
        ### åŠŸèƒ½è¯´æ˜
        
        æœ¬åº”ç”¨å±•ç¤º SysY è¯­è¨€ç¼–è¯‘å™¨å‰ç«¯çš„ä¸‰ä¸ªæ ¸å¿ƒé˜¶æ®µï¼š
        
        1. **è¯æ³•åˆ†æ (Lexical Analysis)**
           - å°†æºä»£ç åˆ†å‰²æˆ Token åºåˆ—
           - è¯†åˆ«å…³é”®å­—ã€æ ‡è¯†ç¬¦ã€å¸¸é‡ã€è¿ç®—ç¬¦
           - æ”¯æŒå…«è¿›åˆ¶(0123)ã€åå…­è¿›åˆ¶(0xFF) æ•°å€¼è½¬æ¢
           - æ£€æµ‹éæ³•å­—ç¬¦ã€æœªé—­åˆæ³¨é‡Šç­‰é”™è¯¯ (Error type A)
        
        2. **è¯­æ³•åˆ†æ (Syntax Analysis)**
           - ä½¿ç”¨é€’å½’ä¸‹é™è§£æå™¨
           - æ ¹æ® SysY æ–‡æ³•æ„å»ºæŠ½è±¡è¯­æ³•æ ‘ (AST)
           - æ£€æµ‹è¯­æ³•ç»“æ„é”™è¯¯ (Error type B)
        
        3. **è¯­ä¹‰åˆ†æ (Semantic Analysis)**
           - å»ºç«‹ç¬¦å·è¡¨ï¼Œç®¡ç†ä½œç”¨åŸŸ
           - æ£€æµ‹å˜é‡æœªå®šä¹‰ã€é‡å¤å®šä¹‰
           - æ£€æµ‹å‡½æ•°æœªå®šä¹‰ã€å‚æ•°ä¸åŒ¹é…
           - æ£€æµ‹ return ç±»å‹ä¸åŒ¹é…
        
        ### æµ‹è¯•ç”¨ä¾‹è¯´æ˜
        
        - **test_01 ~ test_08**: æ­£ç¡®çš„ SysY ç¨‹åº
        - **test_09**: åŒ…å«è¯æ³•é”™è¯¯ (Error type A)
        - **test_10**: åŒ…å«è¯­æ³•é”™è¯¯ (Error type B)
        - **test_11**: åŒ…å«è¯­ä¹‰é”™è¯¯ (Error type 1, 2, 3, 9, 10)
        """
        )


if __name__ == "__main__":
    main()
