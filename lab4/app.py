"""
SysY ç¼–è¯‘å™¨å‰ç«¯å¯è§†åŒ–æ¼”ç¤º

ä½¿ç”¨ Streamlit å±•ç¤ºè¯æ³•åˆ†æã€è¯­æ³•åˆ†æã€è¯­ä¹‰åˆ†æçš„å®Œæ•´æµç¨‹ã€‚
"""

import streamlit as st
import os
from io import StringIO
import sys

from src.lexer import Lexer, LexerError
from src.parser import Parser
from src.ast_nodes import ASTPrinter
from src.semantic_analyzer import SemanticAnalyzer


# é»˜è®¤ç¤ºä¾‹ä»£ç 
DEFAULT_CODE = """// SysY ç¤ºä¾‹ç¨‹åº
int main() {
    int student_id = 20220001;
    int a = 10;
    int b = 20;
    int sum = a + b;
    
    if (sum > 25) {
        return 1;
    } else {
        return 0;
    }
}
"""

# é¢„è®¾æµ‹è¯•ç”¨ä¾‹
TEST_CASES = {
    "åŸºç¡€æµ‹è¯• (test_01)": "test_cases/test_01_basic.sy",
    "ç®—æœ¯è¿ç®— (test_02)": "test_cases/test_02_arithmetic.sy",
    "æ§åˆ¶æµ (test_03)": "test_cases/test_03_control.sy",
    "å‡½æ•°å®šä¹‰ (test_04)": "test_cases/test_04_func.sy",
    "æ•°ç»„æ“ä½œ (test_05)": "test_cases/test_05_array.sy",
    "å¸¸é‡ä¸å…¨å±€ (test_06)": "test_cases/test_06_const_global.sy",
    "æµ®ç‚¹æ•° (test_07)": "test_cases/test_07_float.sy",
    "å¤æ‚ç¨‹åº (test_08)": "test_cases/test_08_complex.sy",
    "è¯æ³•é”™è¯¯ (test_09)": "test_cases/test_09_lex_error.sy",
    "è¯­æ³•é”™è¯¯ (test_10)": "test_cases/test_10_syntax_error.sy",
    "è¯­ä¹‰é”™è¯¯ (test_11)": "test_cases/test_11_semantic_error.sy",
    "å…«è¿›åˆ¶æµ‹è¯•": "test_cases/test_octal.sy",
}


def load_test_file(filepath: str) -> str:
    """åŠ è½½æµ‹è¯•æ–‡ä»¶å†…å®¹"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        return f"// æ— æ³•åŠ è½½æ–‡ä»¶: {e}"


def run_lexer(source_code: str) -> tuple:
    """è¿è¡Œè¯æ³•åˆ†æå™¨ï¼Œè¿”å› (tokens, errors, error_output)"""
    # æ•è·é”™è¯¯è¾“å‡º
    old_stdout = sys.stdout
    sys.stdout = StringIO()

    lexer = Lexer(source_code)
    tokens = []
    errors = []

    try:
        tokens = lexer.tokenize()
        errors = lexer.errors
    except LexerError as e:
        errors.append(str(e))

    error_output = sys.stdout.getvalue()
    sys.stdout = old_stdout

    return tokens, errors, error_output


def run_parser(tokens: list) -> tuple:
    """è¿è¡Œè¯­æ³•åˆ†æå™¨ï¼Œè¿”å› (ast, errors, ast_output, error_output)"""
    old_stdout = sys.stdout
    sys.stdout = StringIO()

    parser = Parser(tokens)
    ast = None
    ast_output = ""

    try:
        ast = parser.parse()
        if not parser.has_error:
            printer = ASTPrinter()
            ast_output = printer.print_ast(ast)
    except Exception as e:
        pass

    error_output = sys.stdout.getvalue()
    sys.stdout = old_stdout

    return ast, parser.errors, ast_output, error_output


def run_semantic(ast) -> tuple:
    """è¿è¡Œè¯­ä¹‰åˆ†æå™¨ï¼Œè¿”å› (success, errors, error_output)"""
    old_stdout = sys.stdout
    sys.stdout = StringIO()

    analyzer = SemanticAnalyzer()
    success = False

    try:
        success = analyzer.analyze(ast)
    except Exception as e:
        pass

    error_output = sys.stdout.getvalue()
    sys.stdout = old_stdout

    return success, analyzer.errors, error_output


def main():
    st.set_page_config(page_title="SysY ç¼–è¯‘å™¨å‰ç«¯æ¼”ç¤º", page_icon="ğŸ”§", layout="wide")

    st.title("ğŸ”§ SysY ç¼–è¯‘å™¨å‰ç«¯æ¼”ç¤º")
    st.caption("å®æ—¶å±•ç¤ºè¯æ³•åˆ†æã€è¯­æ³•åˆ†æã€è¯­ä¹‰åˆ†æçš„å®Œæ•´ç¼–è¯‘æµç¨‹")

    # ä¾§è¾¹æ  - æµ‹è¯•ç”¨ä¾‹é€‰æ‹©
    st.sidebar.header("ğŸ“ æµ‹è¯•ç”¨ä¾‹")
    selected_test = st.sidebar.selectbox("é€‰æ‹©é¢„è®¾æµ‹è¯•ç”¨ä¾‹", ["è‡ªå®šä¹‰è¾“å…¥"] + list(TEST_CASES.keys()))

    # åŠ è½½ä»£ç 
    if selected_test == "è‡ªå®šä¹‰è¾“å…¥":
        initial_code = DEFAULT_CODE
    else:
        filepath = TEST_CASES[selected_test]
        initial_code = load_test_file(filepath)

    # ä¾§è¾¹æ  - åˆ†æé€‰é¡¹
    st.sidebar.header("âš™ï¸ åˆ†æé€‰é¡¹")
    show_lexer = st.sidebar.checkbox("æ˜¾ç¤ºè¯æ³•åˆ†æ", value=True)
    show_parser = st.sidebar.checkbox("æ˜¾ç¤ºè¯­æ³•åˆ†æ", value=True)
    show_semantic = st.sidebar.checkbox("æ˜¾ç¤ºè¯­ä¹‰åˆ†æ", value=True)

    # ä¸»åŒºåŸŸ - ä»£ç ç¼–è¾‘å™¨
    st.subheader("ğŸ“ æºä»£ç ç¼–è¾‘å™¨")
    source_code = st.text_area(
        "SysY æºä»£ç ", value=initial_code, height=300, help="åœ¨æ­¤è¾“å…¥æˆ–ç¼–è¾‘ SysY ä»£ç ï¼Œä¿®æ”¹åè‡ªåŠ¨é‡æ–°åˆ†æ"
    )

    if not source_code.strip():
        st.info("è¯·åœ¨ä¸Šæ–¹è¾“å…¥ SysY ä»£ç ä»¥å¼€å§‹åˆ†æ")
        return

    # è¿è¡Œåˆ†æ
    st.divider()

    # ========== è¯æ³•åˆ†æ ==========
    if show_lexer:
        st.subheader("ğŸ”¤ ä»»åŠ¡ 4.2: è¯æ³•åˆ†æ")

        tokens, lex_errors, lex_error_output = run_lexer(source_code)

        if lex_error_output:
            st.error("è¯æ³•é”™è¯¯ (Error type A)")
            st.code(lex_error_output, language="text")

        if tokens:
            col1, col2 = st.columns([2, 1])

            with col1:
                # Token åˆ—è¡¨
                token_lines = []
                for token in tokens:
                    token_lines.append(token.to_string())

                with st.expander(f"Token åˆ—è¡¨ ({len(tokens)} ä¸ª)", expanded=True):
                    st.code("\n".join(token_lines[:50]), language="text")
                    if len(tokens) > 50:
                        st.caption(f"... è¿˜æœ‰ {len(tokens) - 50} ä¸ª token")

            with col2:
                # ç»Ÿè®¡ä¿¡æ¯
                st.metric("Token æ•°é‡", len(tokens))

                # Token ç±»å‹åˆ†å¸ƒ
                type_count = {}
                for token in tokens:
                    t = token.type.name
                    type_count[t] = type_count.get(t, 0) + 1

                with st.expander("Token ç±»å‹åˆ†å¸ƒ"):
                    for t, count in sorted(type_count.items(), key=lambda x: -x[1])[:10]:
                        st.text(f"{t}: {count}")

        if lex_errors:
            st.warning(f"å‘ç° {len(lex_errors)} ä¸ªè¯æ³•é”™è¯¯")
            return

        st.success("âœ… è¯æ³•åˆ†æå®Œæˆ")
    else:
        tokens, _, lex_error_output = run_lexer(source_code)
        if lex_error_output:
            st.error("è¯æ³•é”™è¯¯ï¼Œæ— æ³•ç»§ç»­")
            st.code(lex_error_output, language="text")
            return

    st.divider()

    # ========== è¯­æ³•åˆ†æ ==========
    if show_parser:
        st.subheader("ğŸŒ³ ä»»åŠ¡ 4.3: è¯­æ³•åˆ†æ")

        ast, parse_errors, ast_output, parse_error_output = run_parser(tokens)

        if parse_error_output:
            st.error("è¯­æ³•é”™è¯¯ (Error type B)")
            st.code(parse_error_output, language="text")

        if ast_output:
            with st.expander("æŠ½è±¡è¯­æ³•æ ‘ (AST)", expanded=True):
                # é™åˆ¶æ˜¾ç¤ºè¡Œæ•°
                lines = ast_output.split("\n")
                if len(lines) > 100:
                    st.code("\n".join(lines[:100]), language="text")
                    st.caption(f"... è¿˜æœ‰ {len(lines) - 100} è¡Œ")
                else:
                    st.code(ast_output, language="text")

        if parse_errors:
            st.warning(f"å‘ç° {len(parse_errors)} ä¸ªè¯­æ³•é”™è¯¯")
            if not show_semantic:
                return
        else:
            st.success("âœ… è¯­æ³•åˆ†æå®Œæˆ")
    else:
        ast, parse_errors, ast_output, parse_error_output = run_parser(tokens)
        if parse_error_output:
            st.error("è¯­æ³•é”™è¯¯ï¼Œæ— æ³•ç»§ç»­è¯­ä¹‰åˆ†æ")
            st.code(parse_error_output, language="text")
            if show_semantic:
                return

    st.divider()

    # ========== è¯­ä¹‰åˆ†æ ==========
    if show_semantic and ast and not parse_errors:
        st.subheader("ğŸ” ä»»åŠ¡ 4.4: è¯­ä¹‰åˆ†æ")

        success, semantic_errors, semantic_error_output = run_semantic(ast)

        if semantic_error_output:
            st.error("è¯­ä¹‰é”™è¯¯")
            st.code(semantic_error_output, language="text")

            # é”™è¯¯ç±»å‹è¯´æ˜
            with st.expander("é”™è¯¯ç±»å‹è¯´æ˜"):
                st.markdown(
                    """
                | é”™è¯¯ç±»å‹ | æè¿° |
                |---------|------|
                | Error type 1 | ä½¿ç”¨æœªå®šä¹‰çš„å˜é‡ |
                | Error type 2 | å˜é‡/å‡½æ•°é‡å¤å®šä¹‰ |
                | Error type 3 | è°ƒç”¨æœªå®šä¹‰çš„å‡½æ•° |
                | Error type 9 | å‡½æ•°å‚æ•°æ•°é‡ä¸åŒ¹é… |
                | Error type 10 | return ç±»å‹ä¸å‡½æ•°è¿”å›ç±»å‹ä¸åŒ¹é… |
                """
                )

        if success:
            st.success("âœ… è¯­ä¹‰åˆ†æå®Œæˆ - ç¨‹åºæ— è¯­ä¹‰é”™è¯¯!")
            st.balloons()
        else:
            st.warning(f"å‘ç° {len(semantic_errors)} ä¸ªè¯­ä¹‰é”™è¯¯")

    # åº•éƒ¨è¯´æ˜
    st.divider()
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
        st.markdown(
            """
        ### åŠŸèƒ½è¯´æ˜
        
        æœ¬åº”ç”¨å±•ç¤º SysY è¯­è¨€ç¼–è¯‘å™¨å‰ç«¯çš„ä¸‰ä¸ªæ ¸å¿ƒé˜¶æ®µï¼š
        
        1. **è¯æ³•åˆ†æ (Lexical Analysis)**
           - å°†æºä»£ç åˆ†å‰²æˆ Token åºåˆ—
           - è¯†åˆ«å…³é”®å­—ã€æ ‡è¯†ç¬¦ã€å¸¸é‡ã€è¿ç®—ç¬¦
           - æ”¯æŒå…«è¿›åˆ¶(0123)ã€åå…­è¿›åˆ¶(0xFF) æ•°å€¼è½¬æ¢
           - æ£€æµ‹éæ³•å­—ç¬¦ã€æœªé—­åˆæ³¨é‡Šç­‰é”™è¯¯ (Error type A)
        
        2. **è¯­æ³•åˆ†æ (Syntax Analysis)**
           - ä½¿ç”¨é€’å½’ä¸‹é™è§£æå™¨
           - æ ¹æ® SysY æ–‡æ³•æ„å»ºæŠ½è±¡è¯­æ³•æ ‘ (AST)
           - æ£€æµ‹è¯­æ³•ç»“æ„é”™è¯¯ (Error type B)
        
        3. **è¯­ä¹‰åˆ†æ (Semantic Analysis)**
           - å»ºç«‹ç¬¦å·è¡¨ï¼Œç®¡ç†ä½œç”¨åŸŸ
           - æ£€æµ‹å˜é‡æœªå®šä¹‰ã€é‡å¤å®šä¹‰
           - æ£€æµ‹å‡½æ•°æœªå®šä¹‰ã€å‚æ•°ä¸åŒ¹é…
           - æ£€æµ‹ return ç±»å‹ä¸åŒ¹é…
        
        ### æµ‹è¯•ç”¨ä¾‹è¯´æ˜
        
        - **test_01 ~ test_08**: æ­£ç¡®çš„ SysY ç¨‹åº
        - **test_09**: åŒ…å«è¯æ³•é”™è¯¯ (Error type A)
        - **test_10**: åŒ…å«è¯­æ³•é”™è¯¯ (Error type B)
        - **test_11**: åŒ…å«è¯­ä¹‰é”™è¯¯ (Error type 1, 2, 3, 9, 10)
        """
        )


if __name__ == "__main__":
    main()
